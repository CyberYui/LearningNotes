# AI绘画Stable Diffusion简易使用

> 一张AI绘图需要以下的步骤 : 搭建可复用环境, 安装可复用模型, 针对模型使用关键词, 生成AI图片, 穿帮修复

- 搭建相关环境一般使用三种方式 :
  1. 拉取GitHub源码部署 ( 可能有很多坑 )
  2. 使用第三方本地整合包 ( 要求有超过3070的电脑, 此类方式要使用所有SD功能 )
  3. 利用云上平台进行搭建

## 云端安装模型

- 本次使用第三种方式, 在阿里云上进行相关搭建和使用

  > 只要从来没有用过函数计算或NAS存储, 即可参与阿里云试用, 但是试用的计算量很有限

- 进入阿里云免费试用专场, 选择 **函数计算FC** 进行3个月的试用, 在工作台里, 创建应用

- 创建角色默认即可, 只是个称呼

- 进入应用菜单, 选择 **通过模板创建应用**, 这里不直接选择 SD, 而是选择人工智能里面的 **AI数字绘画stable-diffusion自定义模型版**, 部署类型选择直接部署, 在此步先不要继续, 先去领取NAS的免费试用, 否则会产生别的费用, NAS用来保存自定义模型

- 同样进入免费试用专栏, 搜索NAS, 找到 **NAS存储** 点击试用, 领取50GB的3个月试用

- 进入刚刚的函数计算控制台, 授权角色, 点一下直接授权刚刚创建的角色即可

- 下面的高级配置中, 地域选择杭州, 镜像地址直接复制下面示例说明中的地址即可, 部署成功后访问域名即可

  > 访问域名时, 需要使用外网下行流量, 这个只有在配置的时候要用, 保持钱包有钱即可

- 点击admin开头的进行相应的环境初始化, 一路下一步即可, 在最后的账号设置处设置好密码即可

- 登录后在可视化界面的地址栏处, 删掉个人空间这里的 `{source:5}` 字样, 直接输入 `/mnt/auto/sd/`, 建议记住这个路径, 这样之后就不需要再设置了

- 在此处展开models文件夹, 点开 Stable-diffusion 文件夹, 这里存放的就是我们的模型, 大模型推荐使用 NeverEnding Dreams ( 即NED ), 在 Stable-diffusion 同层级中可以找到 Lora 文件夹, 这里存放小模型

  > mix4 和 20d 用来画妹子, ss9 用来画帅哥

- 所有模型上传完毕之后, 回到 函数计算FC 的应用菜单, 点击打开访问域名中的另一个域名, 即前端图形化操作界面

- 界面的左上角是大模型名称, 可以通过下拉框选择, 然后在下面选择 **文生图** 标签页, 上面的提示词即正相关词, 下面的反向提示词即为负相关词, 按照需求进行填写即可

- 右侧生成按钮下面有一个像印章(MP3)一样的小图标, 点一下会出现一个选项卡弹出页面, 这里我们选择 **低秩微调模型LoRA** 选项卡, 这里点一个传好的小模型, 比如 mix4, 就会出现尖括号词组, 如 `<lora:mix4:0.8>` 这里第一个参数即代表一个 LoRA, 第二个参数为模型名, 注意模型名要填写模型的card上显示的完整名称, 第三个参数为相关性, 相关性是乘积关系, 小于一是减少相关性, 大于一是增加相关性, 采样器选择 `DPM++ SDE Karras`, 采样步数为 40, 勾选面部修复, 生成种子不要选 -1 即可, 生成完毕后勾选喜欢的图片, 点击局部绘制, 把生成的不喜欢的地方涂黑, 然后微调下随机数, 慢慢的就能生成自己喜欢的图片了

- 使用 LoRA 时, 需要配合提示词, 例如使用 mix4 就需要增加一个 mix4 提示词, 即 `<lora:mix4:0.8>,mix4` 一般在 CivitAI 中, 模型爱好者发布的 LoRA 信息中, 其 Trigger Words 就是相关的提示词

- 这样就搭建好了一个简单的AI绘图模型了, 可以自行炼丹了, 如果要深入, 则观看靠谱的轩轩的视频即可

- 分享作品时, 分享内容为 : 图片 ; 正反关键词 ; 模型信息 ; 其他参数

- 如果自己不知道输入哪些关键词出图, 那可以在 CivitAI 中找到喜欢的图片, 下载好对应的模型, 直接在别人的图片对应内容中复制, 点击生成按钮左下角的小箭头生成图片

### 如何查询免费额度余额

- 进入阿里云 函数计算FC 控制台, 找到概览, 在右下角的资源包处可以看到相应的专项套餐剩余量, 除去免费的部分, 还有两个地方要收费, 一个是磁盘使用费 (不是NAS中模型的费用, 是SD的环境占用, 一般为0.0063元/GB/小时), 另一个是外网下行流量费用/公网出流量 (上传模型不收费, 但是下载计算的流量, 一般为0.5/GB)

  > 我的阿里云余额已经用完了, 阿里云的运行速率低于3060, 且经常卡住, 所以有条件尽量还是自己本地部署使用

-----------------

## win系统傻瓜包

- 网上相关UP有制作一键启动的傻瓜客户端, 只要保证电脑上内存大于64GB, N系显卡, 且显存>8GB, 就可以正常使用了, 好一些的显卡, 例如强于3080的, 可以直接训练相关的AI模型

# 相关的学习内容

- Stable Diffusion 在使用的时候, 大模型, 小模型, 以及相关插件有很多, 下面分别简单介绍一下
- 在生成图片时有不同的参数和属性, 需要了解的基础有 :
  - 正相关提示词 : 
  - 负相关提示词 : 
  - 迭代步数 : 
  - 分辨率 : 
  - 批次和数量 : 

## 模型下载和分类

- 大模型, 一般指大小超过2GB的模型, 一般放在 `根目录\models\Stable-Diffusion` 文件夹下

  > 模型的下载可以有多种方法, 可以找相关的大佬去要, 也可以自己训练, 一般都用下面两种方法下载
  >
  > 1. 可以在网上下载, 比如C站, 即 https://civitai.com 下载模型, 这里的模型都有对应的预览图, 要搜索大模型的话, 勾选筛选里面的 `Checkpoint` 即可, 需要注意该网站是要梯子才能进的, 每个模型里面会有该模型的介绍以及对应的 Tags (即在网站中对应的标签) 和 Trigger Words (即触发该模型的关键词), 每个大模型的介绍需要细看一下, 里面一般会有作者推荐的运行参数
  >
  > 2. 如果使用的是傻瓜包, 那就在模型管理下, 这里下载模型是不需要科学上网的, 找到 `Stable Diffusion模型` 选项卡, 在此处下载即可, 此处不同的选项卡区别见后文, 这种方式唯一的问题就是没有预览图, 像开盲盒一样
  >
  > [PS] 如果对别人生成的图片很好奇, 可以点击对应图片右下角的 `!` 标志, 可以看到对应的正负面词, 但是具体用了那些额外插件就不知道了

- 在 C站 筛选模型的时候, 可以看到有不同的 **Model types**, 比如 `Checkpoint`, 它们的意义和作用都不一样, 简要介绍如下 :

  > - Checkpoint : 一般是指大模型
  > - Textual Inversion : 即 Embedding, 指一部分提示词整合的限定提示词
  > - Hypernetwork : 一般指仅作用于图片风格修改的小模型
  > - Aesthetic Gradient : 
  > - LoRA : 一般是指有特定风格的小模型
  > - LyCORIS : 
  > - Controlnet : 一般指专门用于限定图片人物动作姿势的插件模型
  > - Poses :
  > - Wildcards : 
  > - Other : 

## 网页UI界面及相关笔记

### StableDiffusion模型

- 在最上面的 `Stable Diffusion 模型`, 即 [图片] 这里

- Stable Diffusion 模型这里选择的内容代表当前正在使用的大模型, 如果实时复制了大模型到对应的文件夹内之后, 在网页UI对应内容旁边点击刷新按钮即可刷新大模型

  > 后续有需要时会直接将 Stable Diffusion 简称为 SD

### VAE

- 紧跟大模型之后的 `外挂VAE模型`, 即 [图片] 这里

- VAE 即 Variational autoencoder, 变分自编码器, 作用为滤镜+微调, 其相关原理见后续详解内容

- 加载 VAE 后色彩的饱和度会更高, 但是不同的 VAE 其饱和程度也不一样, 比如下图所示, 可以看到不同 VAE 作用下同一张图片的不同效果

  ![image-20230522211102150](./images/image-20230522211102150.png)

- 需要注意的是, 有的大模型可能自带VAE, 如果再去添加VAE, 可能会直接影响整体出图效果, 但是还是得多试才行

  > VAE 放置位置一般是 `根目录\models\VAE` 文件夹
  >
  > 从C站基本上很难找到VAE, 我们可以直接使用傻瓜包里的模型管理去下载, 即选择 `变分自编码器(VAE)模型` 标签来进行下载, 同样的, 不需要梯子即可正常下载

### Embedding (Textual Inversion)

- Embedding, 又名 Textual Inversion, 即嵌入或文本反转, 这里可以直接理解其为**打包的提示词**

- 我们如果要使用原版SD生成游戏角色, 一般需要特别多的tag去定义才能达到想要的效果

- 但是如果引入了某一特定人物对应的 Embedding 后, 那可以只引用例如 `dva` 这一个提示词, 就实现了对应图片的生成

  > 由于说白了 Embedding 就是一堆 tag 的合集, 故其大小按照一个txt文件的体积看即可, 这样仅为了好记忆, 但是其本质还是一种模型
  >
  > Embedding 的放置位置为 `根目录\embeddings` 文件夹
  >
  > Embedding 的下载还是在 C站, 即筛选时将 `Model types` 选择为 `Textual Inversion` 即可

- Embedding 在界面中不会直接展示, 使用时需要点击 ![image-20230522212747071](./images/image-20230522212747071.png) 按钮来展开对应内容

- 此处的 `嵌入式(T.I. Embedding)` 标签页中会显示对应的 Embedding 模型, 点击一个, 即可将相应的关键词放入正相关提示词中

### LoRA

- LoRA, 全称 Low-Rank Adaptation of Large Language Models, 直译为大语言模型低阶适应

  > 这是微软研究人员为解决大语言模型微调而开发的一项 AI 技术, 其对应的技术实现原理见后续内容

- LoRA 最大的作用就是对人物和物品的复刻, 即只要挂载了对应的 LoRA 模型, 就可以基本 99% 地复刻出指定的人物特征

- LoRA 的适用范围很广泛, 其实它可以实现画风, 画面材质, 特定人脸, 固定人物动作, 特定细节等等功能

- 通俗点来理解, LoRA 几乎可以训练所有与图片相关的内容

  > LoRA 也是在 C站 进行下载, 即筛选时将 `Model types` 选择为 `LoRA` 即可, 需要注意的是下载的时候一定不要用迅雷下载, 会乱码且无法使用, Lora 是一种小模型, 一般放在 `根目录\models\Lora` 文件夹下
  >
  > [注] LoRA 和 embedding 文件的作用看起来类似, 但是其实 embedding 仅仅是一个 tags 的合集小模型文件, 其能够通过定义 tags 实现出多视图等特定功能, 而 LoRA 则是一种被训练过的特性提取模型, 一般针对的是单一内容, 但是深挖了细节, 例如 : embedding 可以看成是简单的模仿, 比如制作网站, embedding 是做出了动态网页, 由于网页量惊人, 有相关的互动效果, 但是实际上并没有后端的数据访问效果, 只模仿到了形 ; 而 LoRA 则是一个前后端整体框架完善的网站, 抄到了精髓, 故一般情况下, 同样效果我们使用 LoRA 更多一些
  
- 在使用 LoRA 时, 需要首先看好要使用的 LoRA 其出图时对应的大模型, 至于某个 LoRA 究竟应该用哪个大模型去生成, 需要在 C站 下载时打开对应的预览图信息, 在此处查看 Model 项内容, 即可找到对应 LoRA 的大模型

  > 大模型和 LoRA 并不是特定组合, 只是不同的大模型针对同一个 LoRA 的生图效果可能不同

- LoRA 在界面中不会直接展示, 使用时需要点击 ![image-20230522212747071](./images/image-20230522212747071.png) 按钮来展开对应内容, 此处 `lora` 标签页中会显示对应的 LoRA 模型, 点击一个模型即可将相应的LoRA 关键词放入正相关 tag 中, 其形式一般为 `<lora:XX:1>`, 尖括号第一个参数即代表要使用 LoRA 模型, 第二个参数是使用的 LoRA 模型的具体名称 (不同版本的 LoRA 其模型名是不一样的, 抄预览图信息的时候也要注意这里), 第三个参数表示使用这个 LoRA 的比重, 或者说是强度值, 权重

  > LoRA 模型的一个首当其冲的问题, 即是其预览图和实际跑出来的图片可能会有买家秀和卖家秀一样的效果, 即生成内容差距很大
  
- 针对使用同一个 LoRA 会导致买家秀和卖家秀的问题, 实际上只是犯了一些基础错误, 以 汉服LoRA(即LoRA为hanfu_v3) 为例, 实际使用时, 尽量参照以下几点来进行 :

  1. **用 LoRA 配套的大模型效果会更好**, LoRA 的训练其实就是参照一个选择的大模型来训练参数的, 如果在使用这个 LoRA 的时候, 没有使用当时训练时使用的大模型, 那当然就会出现不同的效果了, 获取作者 (即 LoRA 发布人) 使用的大模型, 前文也有提到过, 即点开预览图的 `!` 标志来查看对应的 Model 信息, 复制这个模型名, 然后直接在C站筛选`Browsing Mode` 为 `Everything`, 因为你也不知道训练用的大模型是不是XX禁的模型, 然后选择 `Model types` 为 `Checkpoint`, 即搜索大模型, 然后在搜索栏输入复制的名称即可
  2. **最好使用和 LoRA 作者相同的参数**, 即相关生成图片的正负提示词, 然后其他的参数也最好一致, 复制时直接点击信息下面的 `Copy Generation Data`, 在 Stable Diffusion 的前端界面中, 点击 `↙` 按钮即可, 出图的质量一般在这样设置之后, 就和预览图差不多了, 至于如何生成示例图, 则在后续笔记中进行详解
  3. **正确设置 LoRA 的使用权重**, 我们知道调用 LoRA 的时候, 会使用到 `<lora:hanfu_v3:0.9>` 这样的正相关提示词, 这里的第三个数就是权重, 权重越大, 代表其对生成图的影响越大, 权重值尽量不要设置到 1 以上, 不然很容易出效果奇差的图, 即便是十分希望 LoRA 影响出图结果, 也不要将其权重设置为 1, 最好设置为 0.8 或者 0.9, 如果只是想稍微带一点 LoRA 元素作为图片修饰, 那么将其权重设置为 0.4 到 0.6 之间即可
  4. **LoRA 有触发词时一定要使用触发词**, 在一个模型的下载界面, 其 `Trigger Words` 对应的紫色词语就是触发词, 一般将其加入到正向提示词中之后, 在参数一样的情况下, 就能生成相关的图片了, 触发词一般用一个即可, 不同的触发词只是代表的风格不同, 另外提一下对应的 `Tags`, 即在触发词上面的词语, 它们只是代表 LoRA 在 C站 对应的标签, 而和 Stable Diffusion 本身没有任何关系
  5. **新手/第一次使用模型时尽量不要混用 LoRA**, 在使用时, 由于使用者并不了解 LoRA 的训练图集, 如果随意混用则会导致模型相互污染, 出图效果也不会很理想, 即使有时候出现了很惊艳的图片, 也不是调好参数了, 只是碰巧炼丹出丹了而已, 有时候作者会在 LoRA 的简介里说明配合哪些其他 LoRA 使用效果更好, 如果一定要执着于混用 LoRA 的话, 最好还是先看一下其原理, 再去对图片分层次单独设置权重再使用, 这样多个 LoRA 混用不仅有好效果, 而且还有特殊性

  > [注] 以下内容为特殊情况
  >
  > - 有时候作者会疏忽导致没有在任何地方标明 LoRA 炼丹时对应的底模, 这时候就是作者的问题了, 如果实在是想要, 那就在评论区碰碰运气, 看看评论区的人有没有对应的好的效果图, 用他使用的大模型就行了
  > - 有时候作者会疏忽导致没有在模型对应的信息中标明 LoRA 的触发词, 这时一般就是指这个 LoRA 被加载之后, 不需要相应的触发词去作为提示词就能发挥作用, 再者就是作者写到了简介里, 所以最好在下载对应模型时, 将其预览图, 相关信息, 以及简介都保存下来 , 也做好模型的备份

### Hypernetworks

- Hypernetworks, 直译为超网络, 功能和 Embedding, LoRA 类似, 都是对生成的图片进行针对性调整的模型, 将其理解为低配版的 LoRA 即可, 因为其应用领域基本就是为了训练画风, 其训练难度较大, 很有可能会被后出现的 LoRA 淘汰, 但是其现在仍存在的原因是, 其在画面风格的转换上有独到的地方, 比如像素风, Q版风, 这些风格在生产力领域应该都能有所应用

- 虽然 Hypernetworks 最广泛的应用领域是画风, 但是其也可以用于训练人和物品, 通过使用 Hypernetworks 训练, 可以较好地去还原人物(比如蒂法, 毒液等辨识度很高的角色), 而正因神似才导致 Hypernetworks 的发展仅限于此而不是肆无忌惮, 因此现阶段如果还要更高精度的图像生成的话, 还是需要借助 LoRA, 乃至于 Dreambooth 进行协同使用

  > Hypernetworks 也是在 C站 进行下载, 即筛选时将 `Model types` 选择为 `Hypernetwork` 即可,  下载后将其放在 `根目录\models\hypernetworks` 文件夹下即可
  >
  > 如果放入 Hypernetworks 之后没有在 Stable Diffusion 中显示, 点击一下 `刷新` 即可

- Hypernetworks 在界面中不会直接展示, 使用时需要点击 ![image-20230522212747071](./images/image-20230522212747071.png) 按钮来展开对应内容, 然后这里的 `超网络(Hypernetworks)` 标签页中会显示对应的 Hypernetwork 模型, 点击一个, 即可将相应的 Hypernetworks 关键词放入正相关 tag 中, 其形式一般为 `<hypernet:XX:1>`, 尖括号第一个参数即代表要使用 Hypernetwork 模型, 第二个参数是使用的 Hypernetwork 的模型具体名称, 第三个参数表示使用这个 Hypernetwork 的比重, 或者说是强度值, 权重

- 同样的, 正确设置 Hypernetwork 的使用权重也是很重要的, 权重越大, 代表其对生成图的影响越大, 不同于 LoRA, Hypernetwork 的权重并没有固定的范围, 对于不同的 LoRA 来说其权重范围并不一定, 一般以 0.2 为一个梯度去进行出图调试, 直到有一个满意的结果

- 针对同一个 Hypernetwork 实际使用时, 除了上述说明的内容以外, 也尽量参照以下几点来进行, 其实和 LoRA 的使用注意点差不多 :

  1. **用 Hypernetwork 配套的大模型效果会更好** 
  2. **最好使用和 Hypernetwork 作者相同的参数** 
  3. **Hypernetwork 有触发词时一定要使用触发词** 
  4. **新手/第一次使用 Hypernetwork 模型时尽量不要混用** 

---------------

## 其他内容

### 1. 如何辨别陌生文件属于哪一类模型,如何安装和使用

- 在判断一个模型是什么类型的时, 可以借助相关在线工具来进行, 比如 https://spell.novelai.dev/ 这个网站, 也是秋叶大佬制作的, 将下载的模型文件拖进来即可, 绝大多数情况下这种方式是很稳的, 只有极小部分情况才会无法识别模型类型
- 知道模型的类型之后, 就可以根据之前的学习内容去安装和使用了

### 2. 模型的各种后缀和模型类型有什么关系

- 不同类型的模型其后缀可能一样, 不同后缀的模型其类型也可能一样, 可能是pt, ckpt, 甚至是 bin 文件, 由此可见, 不能通过文件的后缀来断定模型的类型
- 不论何种模型文件, 其实都能通过使用压缩软件打开, 打开后就能发现, 不论哪种后缀其实都没有意义, 因为他们都是模型数据, 并没有区别
- 唯一特殊的是 safetensors 文件, 使用压缩文件是无法打开它的, 实际上 safetensors 文件就是一种 ckpt 文件, 在 Stable Diffusion 的 `模型格式转换` 标签中可以将其转换, 转换完之后会发现其实一样
- 不同后缀实际上是为了安全性考虑的, ckpt 文件有可能藏毒, 而经过转化之后的 safetensors 文件就没有这种问题了, 因此下载未知来源的文件时, 尽量下载 safetensors 格式的模型

### 3. 模型的演变极简历史

> 下面的历史其实并不严谨, 只是大致上的情况, 里面有很多的问题, 比如 LoRA 和 Hypernetwork 的演变不仅是原理不同, 而且还有迭代的不同

- 由于历史原因, 造成模型有这么多种不同的后缀的原因是其发展的不同阶段导致的
- 最开始的模型训练成本十分高, AI 实际上从上世纪就开始了, 当时训练一个模型可能需要几千张4090并行训练许久, 并且花费很高的人力物力才生成的, 之后的各种模型实际上都是依照这个大模型去微调的
- 微调需要的成本就不高了, 可能只需要一张 3080, 并且计算不到半天的时间, 就能有很有效的效果, 也就是说一个人可能就能炼模型, 但是对于一个人来说, 这样的成本还是太高了, 于是类似于 embedding 的小模型应运而生, 有时候只需要训练很小的东西时, 比如只训练文本理解的, 那就逐渐演变成了 embedding, 到后面慢慢产生的兼顾效果和文件体积的 LoRA 以及 Hypernetwork, 也导致这些新模型越来越高

------------

## 相关应用

### 1. 让AI模特穿上设计好的衣服/使用指定的商品

- 通过简单的使用商品图, 批量生成高质量的, 动作, 脸型等内容均可定制的, 穿戴/使用着特定商品/产品的模特图片, 这种图片的还原度已经达到 90%~95% 了

### 2. 让已经存在的模特不需要实际使用/穿戴而使用/穿戴指定的商品

- 同上面一样的思路, 这种 AI 出图的训练, 一般使用 4090 显卡的话, 在有充足的产品图片情况下, 训练 15min 模型, 其成图率就有 15% 左右了, 差不多 30s 一张图

  > 这种商业应用的问题也很多, 首先是版权问题, 模型的版权, 甚至是 Stable Diffusion 的版权等内容